{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdaptiveMeteoSrl/meteoGNN/blob/main/LSTM_FINAL.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ztj2c95Io7lg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from math import sqrt\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import os\n",
        "import time\n",
        "import pickle as pc\n",
        "import typing\n",
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
        "from functools import partial\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import datetime\n",
        "from datetime import datetime, timedelta\n",
        "from torch.utils.data import Sampler\n",
        "import sys\n",
        "import csv\n",
        "\n",
        "#Classe che serve a selezionare indici da cui prendere sequenze tali per cui non ci siano misure mancanti\n",
        "class SpecificIndicesSampler(Sampler):\n",
        "    def __init__(self, indices):\n",
        "        self.indices = indices\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "#EarlyStopping è una procedura basata sull'andamento della Loss function\n",
        "#Se la loss function di validazione, per un tot di epoche consecutive, non migliora, allora il training termina\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model, path):\n",
        "        score = -val_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, path)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, path)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model, path):\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "#Metriche varie utili nella fase di training, validation e testing\n",
        "def MAE(pred, true):\n",
        "    return np.mean(np.abs(pred - true))\n",
        "def MSE(pred, true):\n",
        "    return np.mean((pred - true) ** 2)\n",
        "def RMSE(pred, true):\n",
        "    return np.sqrt(MSE(pred, true))\n",
        "def MAPE(pred, true):\n",
        "    return np.mean(np.abs((pred - true) / true))\n",
        "def MSPE(pred, true):\n",
        "    return np.mean(np.square((pred - true) / true))\n",
        "\n",
        "\n",
        "\n",
        "csv_file = sys.argv[1]\n",
        "len_to_predict = sys.argv[2]\n",
        "feature_to_predict = sys.argv[3]\n",
        "station_to_predict = sys.argv[4]\n",
        "\n",
        "last_column = feature_to_predict + \"_STA{}\".format(station_to_predict)\n",
        "print(last_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "xQYNVTL7ApQ-"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(csv_file).dropna()#carico i dati in un dataframe\n",
        "df['DATE'] = pd.to_datetime(df['DATE'])\n",
        "df['hour']=df['DATE'].dt.hour\n",
        "df['month']=df.DATE.dt.month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S26C54Pi5f8-"
      },
      "outputs": [],
      "source": [
        "col0= ['DATE', 'TEMP_STA0', 'TEMP_STA1', 'TEMP_STA2', 'TEMP_STA3', 'TEMP_STA4', 'HUM_STA0',\n",
        "       'HUM_STA1', 'HUM_STA2', 'HUM_STA3', \"HUM_STA4\", 'PRESS_STA0',\n",
        "       'PRESS_STA1', 'PRESS_STA2', 'PRESS_STA3', 'PRESS_STA4', 'PRO_X_STA0',\n",
        "       'PRO_X_STA1', 'PRO_X_STA2', 'PRO_X_STA3', 'PRO_X_STA4', 'PRO_Y_STA0',\n",
        "       'PRO_Y_STA1', 'PRO_Y_STA2', 'PRO_Y_STA3', 'PRO_Y_STA4', 'hour', 'month']#seleziono le colonne relative alle features da utilizzare\n",
        "\n",
        "\n",
        "\n",
        "df=df[col0]\n",
        "\n",
        "df[\"PRED_COL\"] = df[last_column]\n",
        "df.drop(columns = last_column, inplace = True)\n",
        "df.rename(columns={\"PRED_COL\": last_column}, inplace=True)\n",
        "co = df.columns[1:]\n",
        "df = df.reset_index(drop=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oMoRqZvz5piW"
      },
      "outputs": [],
      "source": [
        "#Qui si esegue il controllo sui dati mancanti\n",
        "#Si chiede che di una sequenza di dati presa dal dataframe, questa non presenti dati mancanti\n",
        "\n",
        "desired_interval = 1008                 #Deve essere più maggiore o uguale alla lunghezza della sequenza in input più il tempo nel futuro da prevedere (cioè desired_interval >= seq_len + pred_len)\n",
        "df[\"DATE\"]=pd.to_datetime(df[\"DATE\"])\n",
        "desired_duration = timedelta(days=7)\n",
        "good_starts = []\n",
        "for i in range(df.shape[0]-desired_interval):                                      #Crea vettore di indici buoni per selezionare sequenze senza salti\n",
        "  if df[\"DATE\"][i+desired_interval]-df[\"DATE\"][i] == desired_duration:\n",
        "    good_starts.append(df.index[i])\n",
        "fea = df.shape[1]-1\n",
        "good_starts_train = good_starts[0:int(len(good_starts)*0.7)]            #Divide il vettore good_starts in indici iniziali di train, validation e di test\n",
        "good_starts_vali = good_starts[int(len(good_starts)*0.7):int(len(good_starts)*0.9)]\n",
        "good_starts_vali = [l for l in good_starts_vali if l>(good_starts_train[-1]+desired_interval+1)]\n",
        "good_starts_test = good_starts[int(len(good_starts)*0.9):int(1*len(good_starts))]\n",
        "good_starts_test = [l for l in good_starts_test if l>(good_starts_vali[-1]+desired_interval+1)]  #Train, validation e test non possono sovrapporsi, dunque c'è un embargo fra i tre set\n",
        "\n",
        "shuffled_list = good_starts_train.copy()                                                        #Fa lo shuffle del train\n",
        "random.shuffle(shuffled_list)\n",
        "tr = shuffled_list\n",
        "va = good_starts_vali\n",
        "te =  good_starts_test\n",
        "gs=[tr,va,te]                                            #gs è un array che contiene a sua volta tre array di indici, uno con indici da cui leggere sequenze per train, e gli altri due per vali e test\n",
        "df=df[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "lW0P1PJ6qFJd"
      },
      "outputs": [],
      "source": [
        "#'forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
        "#Per l'obbiettivo preposto si utilizzano più features per prevederne una sola, per cui l'opzione usata è sempre e solo MS.\n",
        "class Dataset():\n",
        "    def __init__(self, root_path=None, flag='train', size=None,\n",
        "                 features='MS',target=last_column, scale=True, timeenc=0, freq='10m'):\n",
        "\n",
        "        # size [seq_len, label_len, pred_len]\n",
        "        # info sugli input\n",
        "        # size:lista contenente [seq_len, label_len, pred_len]\n",
        "        # features: stringa che specifica il tipo di task di previsione\n",
        "        # target: stringa che specifica la variabile target\n",
        "        # scale: se True si effettua lo scaling dei dati con MinMax Scaler, altrimenti False\n",
        "        # timeenc: intero indicante il tipo di encoding temporale\n",
        "        # freq: stringa che specifica la frequenza\n",
        "        if size == None:\n",
        "            print(\"size è none. Forse ci sono problemi\")\n",
        "        else:\n",
        "            self.seq_len = size[0]\n",
        "            self.pred_len = size[1]\n",
        "        # init\n",
        "        assert flag in ['train', 'val','test']\n",
        "        type_map = {'train': 0, 'val': 1, 'test':2} #Flag che specifica il tipo di splitting diverso per ogni set\n",
        "        self.set_type = type_map[flag]\n",
        "\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.scale = scale\n",
        "        self.timeenc = timeenc\n",
        "        self.freq = freq\n",
        "        self.root_path = root_path\n",
        "        self.__read_data__()\n",
        "\n",
        "    def __read_data__(self): #Questo metodo effettua una lettura e un preprocessing dei dati (ossia uno scaling)\n",
        "        self.scaler = MinMaxScaler() #Scaler che normalizza i dati tra 0 e 1\n",
        "        df_raw = df\n",
        "        #border1s e border2s sono liste usate per definire gli estremi degli intervalli sulla base del tipo di splitting\n",
        "        #in questo caso non è necessario definire diversi borders per train, validation e test in quanto gli indici da cui\n",
        "        #prendere le sequenze sono già specificati nel Sampler (classe all'inizio del codice)\n",
        "\n",
        "        #Nel caso si volesse non utilizzare il sampler sarebbe tuttavia necessario specificare i borders sulla base della divisione in train, validation e test\n",
        "        border1s = [0,\n",
        "                    0,\n",
        "                    0]\n",
        "        border2s = [int(len(df_raw)*1),\n",
        "                    int(len(df_raw)*1),\n",
        "                    int(len(df_raw)*1)]\n",
        "\n",
        "        border1 = border1s[self.set_type]\n",
        "        border2 = border2s[self.set_type]\n",
        "\n",
        "\n",
        "        if self.features == 'M' or self.features == 'MS':\n",
        "            cols_data = df_raw.columns[1:]\n",
        "            df_data = df_raw[cols_data]\n",
        "\n",
        "        elif self.features == 'S':\n",
        "            df_data = df_raw[[self.target]]\n",
        "\n",
        "        if self.scale:\n",
        "            stop_index = good_starts_train[-1]+desired_interval\n",
        "            train_data = df_data.iloc[:stop_index + 1]\n",
        "            self.scaler.fit(train_data.values)\n",
        "            data = self.scaler.transform(df_data.values)\n",
        "            self.scaler.fit(train_data[self.target].values.reshape(-1, 1)) #Cosi' poi da andare a denormalizzare le previsioni per valutare le prestazioni sul test\n",
        "        else:\n",
        "            data = df_data.values\n",
        "        self.data_x = data[border1:border2]\n",
        "        self.data_y = data[border1:border2]\n",
        "\n",
        "\n",
        "    def __getitem__(self, index): #Metodo usato per recuperare una specifica sequenza di dati dal dataset dato un certo indice (parte, insieme al Sampler, del DataLoader)\n",
        "        s_begin = index\n",
        "        s_end = s_begin + self.seq_len\n",
        "        seq_x = self.data_x[s_begin:s_end]\n",
        "        seq_y = self.data_y[s_begin:s_end+self.pred_len]\n",
        "\n",
        "        return seq_x,seq_y\n",
        "\n",
        "    def __len__(self): #Ritorna il numero totale di sequenze nel dataset\n",
        "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
        "\n",
        "    def inverse_transform(self, data): # Effettua lo scaling inverso dei dati nel caso si sia scelto di riscalarli nel preprocessing (si fa per il test)\n",
        "        return self.scaler.inverse_transform(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "K-4FVjLOC81Z"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    \"\"\"Layer comprising a convolution layer followed by LSTM and dense layers.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        seq_len,\n",
        "        pred_len,\n",
        "        n_features,\n",
        "        batch_size,\n",
        "        hidden,\n",
        "        nlayers,\n",
        "        dropout\n",
        "    ):\n",
        "        super(Model, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len=pred_len\n",
        "        self.hidden=hidden\n",
        "        self.n_features=n_features\n",
        "        self.batch_size=batch_size\n",
        "        self.nlayers=nlayers\n",
        "        self.dropout = dropout                           #Modello con layer LSTM e batch normalization alternati\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(self.n_features)\n",
        "        self.lstm1 = nn.LSTM(self.n_features, self.hidden, num_layers=self.nlayers, batch_first=True, dropout=self.dropout)\n",
        "        self.bn2 = nn.BatchNorm1d(self.hidden)\n",
        "        self.lstm2 = nn.LSTM(self.hidden, self.hidden * 2, num_layers=self.nlayers, batch_first=True, dropout=self.dropout)\n",
        "        self.bn3 = nn.BatchNorm1d(self.hidden * 2)\n",
        "        self.dense1 = nn.Linear(self.hidden * 2, self.hidden * 1)\n",
        "        self.bn5 = nn.BatchNorm1d(self.hidden * 1)\n",
        "        self.dense2 = nn.Linear(self.hidden * 1, 1)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"input: BxSxF\n",
        "        con B=Batch_size\n",
        "            S=Lunghezza sequenza\n",
        "            F=Numero di features\n",
        "        \"\"\"\n",
        "        inputs=self.bn1(inputs.permute(0,2,1)).permute(0,2,1)                #I permute servono per fare le batch normalization appropriatamente\n",
        "        lstm1_out,_ = self.lstm1(inputs)\n",
        "        lstm1_out=self.bn2(lstm1_out.permute(0,2,1)).permute(0,2,1)\n",
        "        lstm2_out,_ = self.lstm2(lstm1_out)\n",
        "        lstm2_out=self.bn3(lstm2_out.permute(0,2,1)).permute(0,2,1)\n",
        "        dense1_out = self.dense1(lstm2_out)\n",
        "        dense1_out=self.bn5(dense1_out.permute(0,2,1)).permute(0,2,1)\n",
        "        dense2_out = self.dense2(dense1_out)\n",
        "        dense2_out = dense2_out[:, -self.pred_len:,:]        #Alla fine si seleziona soltanto l'ultimo elemento in uscita dal layer LSTM\n",
        "        return dense2_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ncMo7ZYcslKO"
      },
      "outputs": [],
      "source": [
        "class DNNModel(object):\n",
        "    def __init__(self,batch_size,seq_len,lstm_units,lr,weight_decay, nlayers,dropout,epochs_early_stopping=20, pred_len = len_to_predict):\n",
        "        self.embed='fixed'\n",
        "        self.batch_size=batch_size\n",
        "        self.freq='10m'\n",
        "        self.pred_len= pred_len   #Questo è un input da riga di comando\n",
        "        self.seq_len=seq_len\n",
        "\n",
        "        self.n_features=27       #Features in ingresso\n",
        "        self.lstm_units=lstm_units       #Dimensione hidden di output per LSTM\n",
        "        self.train_epochs=500                #Numero epoche\n",
        "        self.patience_early_stopping = 10    #Pazienza EarlyStopping\n",
        "        self.patience_scheduler = 2          #Pazienza scheduler (Spiegato dopo)\n",
        "        self.features='MS'\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.checkpoints='./checkpoints/'  #Serve a salvare i pesi relativi alla migliore prestazione del modello\n",
        "        self.lr=lr                         #Learning rate\n",
        "        self.nlayers=nlayers                #Numero di stacks di lstm (ogni layer lstm può processare le sequenze su più livelli, andando a carpire correlazioni più sottili)\n",
        "        self.weight_decay = weight_decay    #Serve a tenere sotto controllo overfitting riducendo i pesi man mano che il training avanza\n",
        "        self.dropout=dropout                #Serve a prevenire overfitting annullando l'effetto di alcuni pesi a caso in training\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def data_provider(self,flag):\n",
        "        data_dict = {'ETTh1': Dataset}    #Dizionario che serve a prendere il dataset\n",
        "        Data = data_dict['ETTh1']\n",
        "\n",
        "        if flag == 'val':            #Condizioni poste sulla base della fase dell'algoritmo (per train, validation, test)\n",
        "            shuffle_flag = False\n",
        "            drop_last =  True\n",
        "            batch_size =  1\n",
        "            freq = self.freq\n",
        "            v_s = 1                     #Indice vs serve a selezionare una serie di indici da usare dal vettore gs per il Sampler\n",
        "\n",
        "        elif flag == 'test':\n",
        "            shuffle_flag = False\n",
        "            drop_last =  True\n",
        "            batch_size =  1\n",
        "            freq = self.freq\n",
        "            v_s = 2\n",
        "\n",
        "        else:\n",
        "            shuffle_flag= False\n",
        "            drop_last =  True\n",
        "            batch_size =self.batch_size\n",
        "            freq = self.freq\n",
        "            v_s = 0\n",
        "\n",
        "        data_set = Data(\n",
        "            flag=flag,\n",
        "            size=[self.seq_len, self.pred_len],         #Definizione del dataset e del DataLoader\n",
        "            freq=self.freq\n",
        "        )\n",
        "\n",
        "        data_loader = DataLoader(\n",
        "            data_set,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=shuffle_flag,\n",
        "            num_workers=0,\n",
        "            drop_last=drop_last,\n",
        "            sampler=SpecificIndicesSampler(gs[v_s])     #Sampler accede al vettore di indici buoni che ho definito (o train o validation) e sceglie indici da li cosi da non avere salti\n",
        "            )                                           #in una singola sequenza. Non si può mettere Sampler e shuffle insieme, quindi lo shuffle è effettuato prima\n",
        "        return data_set, data_loader\n",
        "\n",
        "    #Crea il modello da addestrare\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Model(seq_len=self.seq_len,pred_len=self.pred_len,n_features=self.n_features,batch_size=self.batch_size, hidden=self.lstm_units,nlayers=self.nlayers,dropout=self.dropout).to(self.device)\n",
        "        return model\n",
        "\n",
        "    #Seleziona ottimizzatore\n",
        "    def _select_optimizer(self):\n",
        "        model_optim = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        return model_optim\n",
        "\n",
        "    #Seleziona la funzione di loss\n",
        "    def _select_criterion(self):\n",
        "        criterion = nn.MSELoss() # Altrimenti nn.HuberLoss(reduction='mean', delta=1.0), altrimenti nn.MSELoss()\n",
        "        return criterion\n",
        "\n",
        "    def _get_data(self, flag):\n",
        "        data_set, data_loader = self.data_provider(flag)\n",
        "        return data_set, data_loader\n",
        "\n",
        "\n",
        "    #Fase di validation\n",
        "\n",
        "    def vali(self, vali_data, vali_loader, criterion):\n",
        "        total_loss = []\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "          for i, (batch_x,batch_y) in enumerate(vali_loader):\n",
        "            batch_x = batch_x.float().to(self.device)\n",
        "            batch_y = batch_y.float().to(self.device)\n",
        "            outputs = self.model(batch_x)\n",
        "            f_dim = -1 if(self.features == 'MS' or self.features =='S')  else 0\n",
        "            outputs = outputs[:, -1:, f_dim:]     #Prende ultimo elemento predetto (in realtà questo è gia fatto nella rete) dall'output della rete\n",
        "            batch_y = batch_y[:, -1:, f_dim:].to(self.device)  #Prende ultimo valore della sequenza, vale a dire quello che si vuole prevedere, dalla sequenza presa dal dataset di validation\n",
        "            pred = outputs.detach().cpu()\n",
        "            true = batch_y.detach().cpu()\n",
        "            loss = criterion(pred, true)\n",
        "\n",
        "            total_loss.append(loss)\n",
        "\n",
        "        total_loss = np.average(total_loss)\n",
        "        self.model.train()\n",
        "        return total_loss\n",
        "\n",
        "    #Fase di train\n",
        "\n",
        "    def train(self,setting):\n",
        "        train_data, train_loader = self._get_data(flag='train')\n",
        "        vali_data, vali_loader = self._get_data(flag='val')\n",
        "        test_data, test_loader = self._get_data(flag='test')\n",
        "        path = os.path.join(self.checkpoints, setting)\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "        time_now = time.time()\n",
        "        train_steps = len(train_loader)\n",
        "        early_stopping = EarlyStopping(patience=self.patience_early_stopping, verbose=True)\n",
        "        model_optim = self._select_optimizer()\n",
        "        criterion = self._select_criterion()  #Utilizzo delle funzioni precedentemente definite per dare al modello impostazioni di training\n",
        "\n",
        "        #Scheduler decide di abbassare il learning rate (cioè di permettere meno mobilità ai pesi) quando c'è uno stallo nella validation loss\n",
        "        scheduler = ReduceLROnPlateau(model_optim, mode='min', patience=self.patience_scheduler, factor=0.5, verbose=False)\n",
        "\n",
        "        #Training effettivo\n",
        "        for epoch in range(self.train_epochs):\n",
        "            iter_count = 0\n",
        "            train_loss = []\n",
        "            self.model.train()\n",
        "            epoch_time = time.time()\n",
        "            for i, (batch_x, batch_y) in enumerate(train_loader):\n",
        "                iter_count += 1\n",
        "                model_optim.zero_grad()                                   #Zero grad, loss backward, optimizer step sono gli elementi essenziali nella fase di training in torch\n",
        "                batch_x = batch_x.float().to(self.device)                 #In validation e in test NON VENGONO MESSI, perche i pesi sono fissi\n",
        "                batch_y = batch_y.float().to(self.device)\n",
        "                outputs = self.model(batch_x)\n",
        "                f_dim = -1 if(self.features == 'MS' or self.features =='S')  else 0    #Tutto simile a ciò che è stato definito in fase di validation\n",
        "                outputs = outputs[:, -1:, f_dim:]\n",
        "                batch_y = batch_y[:, -1:, f_dim:].to(self.device)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                train_loss.append(loss.item())\n",
        "\n",
        "                if (i + 1) % 10000 == 0:\n",
        "                    print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
        "                    speed = (time.time() - time_now) / iter_count\n",
        "                    left_time = speed * ((self.train_epochs - epoch) * train_steps - i)\n",
        "                    print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
        "                    iter_count = 0\n",
        "                    time_now = time.time()\n",
        "\n",
        "                loss.backward()\n",
        "                model_optim.step()\n",
        "\n",
        "            print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
        "            train_loss = np.average(train_loss)\n",
        "            vali_loss= self.vali(vali_data, vali_loader, criterion)                    #Valutazione di validation e di test\n",
        "            scheduler.step(vali_loss)\n",
        "            test_loss = self.vali(test_data, test_loader, criterion)\n",
        "\n",
        "            print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
        "                epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n",
        "\n",
        "            early_stopping(vali_loss, self.model, path)\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "        best_model_path = path + '/' + 'checkpoint.pth'\n",
        "        self.model.load_state_dict(torch.load(best_model_path))\n",
        "        return self.model\n",
        "\n",
        "\n",
        "\n",
        "    #Fase di testing indipendente, ossia post-training\n",
        "    #La valutazione di test è stata già fatta per ogni epoca, tuttavia dopo la fine del training, per visualizzare i risultati, si prende il miglior modello salvato\n",
        "    #ossia quello che ha ottenuto la minore loss di validation (non si può decidere sulla base del test in quanto il programma non deve vedere il test in alcun modo)\n",
        "    def vali_test(self, setting, test=True):\n",
        "        test_data, test_loader = self._get_data(flag='test')\n",
        "\n",
        "        if test:\n",
        "            print('loading model')\n",
        "            self.model.load_state_dict(torch.load(os.path.join('./checkpoints/' + setting, 'checkpoint.pth')))  #Carica sul modello i pesi con migliore validation loss\n",
        "\n",
        "        preds = []\n",
        "        trues = []\n",
        "        folder_path = './test_results/' + setting + '/'\n",
        "        if not os.path.exists(folder_path):\n",
        "            os.makedirs(folder_path)\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (batch_x, batch_y) in enumerate(test_loader):\n",
        "              batch_x = batch_x.float().to(self.device)\n",
        "              batch_y = batch_y.float().to(self.device)\n",
        "              outputs = self.model(batch_x)\n",
        "              f_dim = -1 if(self.features == 'MS' or self.features =='S')  else 0\n",
        "              outputs = outputs[:, -1:, f_dim:]\n",
        "              batch_y = batch_y[:, -1:, f_dim:].to(self.device)\n",
        "              outputs = outputs.detach().cpu().numpy()\n",
        "              batch_y = batch_y.detach().cpu().numpy()\n",
        "              pred = outputs\n",
        "              true = batch_y\n",
        "              preds.append(pred)\n",
        "              trues.append(true)\n",
        "\n",
        "\n",
        "        preds = np.array(preds)                                                #Genera i risultati da poter plottare e visualizzare\n",
        "        trues = np.array(trues)\n",
        "\n",
        "        preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n",
        "        trues = trues.reshape(-1, trues.shape[-2], trues.shape[-1])\n",
        "\n",
        "\n",
        "        # result save\n",
        "        folder_path = './results/' + setting + '/'\n",
        "        if not os.path.exists(folder_path):\n",
        "            os.makedirs(folder_path)\n",
        "\n",
        "        return preds,trues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Gh8bdZAjs28Q"
      },
      "outputs": [],
      "source": [
        "def _build_space(): #La funzione genera lo spazio di ricerca per gli iperaparametri da usare nel fine-tuning\n",
        "    space = {\n",
        "        'batch_size': hp.quniform('batch_size', 16, 526, 16),\n",
        "        'seq_len': hp.quniform('seq_len', 10, 100, 1),\n",
        "        'lstm_units': hp.quniform('lstm_units', 16, 256, 16),\n",
        "        'lr': hp.uniform('lr', 0.000000001, 0.01),\n",
        "        'nlayers': hp.quniform('nlayers', 1, 2, 1),\n",
        "        'dropout': hp.uniform('dropout', 0, 0.3),\n",
        "        \"weight_decay\": hp.uniform('weight_decay', 0.000000001, 0.0001),\n",
        "        }\n",
        "    return space\n",
        "\n",
        "\n",
        "\n",
        "def _hyperopt_objective(hyperparameters, trials, trials_file_path, max_evals):#definisce l'obiettivo della minimizzazione per il processo di ottimizzazione degli iperparametri\n",
        "    #info input:\n",
        "    #hyperparameters: dizionario contenente gli iperparametri da valutare\n",
        "    #trials:oggetto che conserva le informazioni rilevanti per il processo di ottimizzazione\n",
        "    #trials_file_path: path del file in cui salvare i trials\n",
        "    #max_evals: numero di valutazioni massimo della funzione di costo\n",
        "    pc.dump(trials, open(trials_file_path, \"wb\"))\n",
        "    setting = '{}'.format('EMANUELE')\n",
        "    print(hyperparameters)\n",
        "    forecaster = DNNModel(batch_size=int(hyperparameters['batch_size']),seq_len=int(hyperparameters['seq_len']),weight_decay=int(hyperparameters['weight_decay']),lstm_units=int(hyperparameters['lstm_units']),nlayers=int(hyperparameters['nlayers']),lr=(hyperparameters['lr']),dropout=(hyperparameters['dropout']))\n",
        "\n",
        "    forecaster.train(setting).to(\"cuda\")\n",
        "    Yp_mean ,Y_test= forecaster.vali_test(setting,test=True)\n",
        "    Y_test = Dataset(flag='test',size=[1,1,1],freq='10m').inverse_transform(Y_test.reshape(-1, Y_test.shape[-1])).flatten()\n",
        "    Yp_mean = Dataset(flag='test',size=[1,1,1],freq='10m').inverse_transform(Yp_mean.reshape(-1, Yp_mean.shape[-1])).flatten()\n",
        "\n",
        "    mae_validation = np.mean(MAE(Yp_mean, Y_test))#calcola la media del mae sul validation set e sul test set\n",
        "    smape_validation = np.mean(RMSE(Yp_mean, Y_test))\n",
        "    differenza=(abs(Yp_mean-Y_test)).flatten()#calcola gli errori in modulo sul test set\n",
        "    print(\"errore max\", max(differenza))#printa il massimo e il minimo dell'errore commesso\n",
        "    print(\"errore min\", min(differenza))\n",
        "    print(\"  MAE: {:.3f} | RMSE: {:.3f} %\".format(mae_validation, smape_validation))\n",
        "    return_values = {'loss': mae_validation, 'MAE test': mae_validation,'RMSE test': smape_validation, 'hyper': hyperparameters,'status': STATUS_OK}#i risultati del processo vengono ritornati tramite un dizionario\n",
        "    if trials.losses()[0] is not None:#la condizione controlla che nell'oggetto trials ci siano effettivamente delle losses. Trials.losses() riporta la lista di loss salvate nel processo di ottimizzazione\n",
        "        MAEVal = trials.best_trial['result']['MAE test']#riporta il miglior valore del MAE durante il processo di ottimizzazione.\n",
        "        sMAPEVal = trials.best_trial['result']['RMSE test']#riporta il miglior valore del RMSE durante il processo di ottimizzazione\n",
        "        parametri = trials.best_trial['result']['hyper']#riporta la migliore scelta di iperparametri\n",
        "\n",
        "        print('\\n\\nTested {}/{} iterations.'.format(len(trials.losses()) - 1,max_evals))\n",
        "        print('Best MAE - Validation Dataset')\n",
        "        print(\"  MAE: {:.3f} | RMSE: {:.3f} %\".format(MAEVal, sMAPEVal))\n",
        "    return return_values\n",
        "\n",
        "def hyperparameter_optimizer(path_hyperparameters_folder=os.path.join('.', 'experimental_files'),\n",
        "                             new_hyperopt=1, max_evals=3):\n",
        "\n",
        "    if not os.path.exists(path_hyperparameters_folder):\n",
        "        os.makedirs(path_hyperparameters_folder)\n",
        "    trials_file_name = 'DNN_hyperparameters'\n",
        "    trials_file_path = os.path.join(path_hyperparameters_folder, trials_file_name)\n",
        "    if new_hyperopt:#Se new_hyperopt è True inizializza un nuovo oggetto trials\n",
        "        trials = Trials()\n",
        "    else:\n",
        "        trials = pc.load(open(trials_file_path, \"rb\"))\n",
        "    space = _build_space()\n",
        "\n",
        "    fmin_objective = partial(_hyperopt_objective, trials=trials, trials_file_path=trials_file_path,\n",
        "                             max_evals=max_evals)\n",
        "    fmin(fmin_objective, space=space, algo=tpe.suggest, max_evals=max_evals, trials=trials, verbose=False)#fmin da hyperopt performa l'ottimizzazione utilizzando l'algoritmo Tree-structured Parzen Estimators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdZSRLlltdPJ"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "new_hyperopt = 1\n",
        "max_evals = 35\n",
        "path_hyperparameters_folder = \"./experimental_files/\"\n",
        "best_hyperparameters=hyperparameter_optimizer(path_hyperparameters_folder=path_hyperparameters_folder,new_hyperopt=new_hyperopt, max_evals=max_evals)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trials_file_name = 'DNN_hyperparameters'\n",
        "trials_file_path = os.path.join(path_hyperparameters_folder, trials_file_name)\n",
        "trials = pc.load(open(trials_file_path, \"rb\"))\n",
        "for trial in trials.trials:\n",
        "    print(trial['result'])"
      ],
      "metadata": {
        "id": "tNTK2CwIQWbC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}