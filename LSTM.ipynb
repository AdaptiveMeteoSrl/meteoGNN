{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QSevVxZCK4zA"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Model(nn.Module):\n",
        "    \"\"\"Layer comprising a convolution layer followed by LSTM and dense layers.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        seq_len,\n",
        "        pred_len,\n",
        "        n_features,\n",
        "        batch_size,\n",
        "        hidden,\n",
        "        nlayers,\n",
        "        dropout\n",
        "    ):\n",
        "        super(Model, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len=pred_len\n",
        "        self.hidden=hidden\n",
        "        self.n_features=n_features\n",
        "        self.batch_size=batch_size\n",
        "        self.nlayers=nlayers\n",
        "        self.dropout = dropout                           #definiamo il modello con le relative batch normalizzation.\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(self.n_features)\n",
        "        self.lstm1 = nn.LSTM(self.n_features, self.hidden, num_layers=self.nlayers, batch_first=True, dropout=self.dropout)\n",
        "        self.bn2 = nn.BatchNorm1d(self.hidden)\n",
        "        self.lstm2 = nn.LSTM(self.hidden, self.hidden * 2, num_layers=self.nlayers, batch_first=True, dropout=self.dropout)\n",
        "        self.bn3 = nn.BatchNorm1d(self.hidden * 2)\n",
        "        self.dense1 = nn.Linear(self.hidden * 2, self.hidden * 1)\n",
        "        self.bn5 = nn.BatchNorm1d(self.hidden * 1)\n",
        "        self.dense2 = nn.Linear(self.hidden * 1, 1)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"input: BxSxF\n",
        "        con B=Batch_size\n",
        "            S=Lunghezza sequenza\n",
        "            F=Numero di features\n",
        "        \"\"\"\n",
        "        inputs=self.bn1(inputs.permute(0,2,1)).permute(0,2,1)\n",
        "        lstm1_out,_ = self.lstm1(inputs)\n",
        "        lstm1_out=self.bn2(lstm1_out.permute(0,2,1)).permute(0,2,1)\n",
        "        lstm2_out,_ = self.lstm2(lstm1_out)\n",
        "        lstm2_out=self.bn3(lstm2_out.permute(0,2,1)).permute(0,2,1)\n",
        "        dense1_out = self.dense1(lstm2_out)\n",
        "        dense1_out=self.bn5(dense1_out.permute(0,2,1)).permute(0,2,1)\n",
        "        dense2_out = self.dense2(dense1_out)\n",
        "        dense2_out = dense2_out[:, -self.pred_len:,:]\n",
        "        return dense2_out"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kXPpl-8hLa7p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}